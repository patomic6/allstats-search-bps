{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3346378c",
   "metadata": {},
   "source": [
    "# 1. Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962cc90",
   "metadata": {},
   "source": [
    "Terdapat penyesuaian mapping dari sistem existing, dengan tidak menyertakan field dalam bahasa inggris, karena model multilingual telah mampu mengatasi pencarian lintas bahasa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Inisialisasi koneksi Elasticsearch\n",
    "es = Elasticsearch(\"http://10.100.244.126:9200\")\n",
    "\n",
    "mapping = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"synonym_filter\": {\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms_path\": \"analysis/synonyms.txt\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"synonym_analyzer\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"synonym_filter\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"text\"},\n",
    "      \"konten\": {\"type\": \"keyword\"},\n",
    "      \"jenis\": {\"type\": \"keyword\"},\n",
    "      \"judul\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"synonym_analyzer\"\n",
    "      },\n",
    "      \"deskripsi\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"synonym_analyzer\"\n",
    "      },\n",
    "      \"title_embeddings_384\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True\n",
    "      },\n",
    "      \"mfd\": {\"type\": \"keyword\"},\n",
    "      \"tgl_rilis\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"last_update\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"source\": {\"type\": \"keyword\"},\n",
    "      \"url\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Nama indeks\n",
    "index_name = \"datacontent\"\n",
    "\n",
    "# Hapus indeks jika sudah ada\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Indeks '{index_name}' lama dihapus.\")\n",
    "\n",
    "# Buat indeks baru dengan mapping\n",
    "try:\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"Indeks '{index_name}' berhasil dibuat.\")\n",
    "except Exception as e:\n",
    "    print(\"Gagal membuat indeks:\", e)\n",
    "\n",
    "# (Opsional) Verifikasi mapping\n",
    "try:\n",
    "    current_mapping = es.indices.get_mapping(index=index_name)\n",
    "    print(\"Mapping saat ini:\", current_mapping)\n",
    "except Exception as e:\n",
    "    print(\"Gagal mendapatkan mapping:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403f31b",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc259b4",
   "metadata": {},
   "source": [
    "Inti perubahan indexing dari sistem existing adalah penambahan proses encoding text menjadi dense_vector dan menyimpan hasil encoding tersebut.\n",
    "Pada contoh berikut, dilakukan indexing dari sumber webapi bps. Sumber data dapat disesuaikan dengan kondisi dan perubahan aktual di sistem produksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6dae9",
   "metadata": {},
   "source": [
    "## Indexing dari webapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ebc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ElasticsearchIndexer:\n",
    "    def __init__(self, es_host='http://127.0.0.1:9200', index_name='datacontent', bypass_date_filter=False):\n",
    "        self.es = Elasticsearch([es_host])\n",
    "        self.index_name = index_name\n",
    "        self.batch_size = 2500\n",
    "        self.api_key = '7a62d6af2de1e805bc5f44d8a0f6ad17'  # Replace with your BPS API key\n",
    "        self.embedding_model = SentenceTransformer('yahyaabd/allstats-search-mini-v1-1-mnrl')\n",
    "        self.domain_names = self.fetch_domain_names()\n",
    "        self.bypass_date_filter = bypass_date_filter\n",
    "        self.ensure_index()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        \"\"\"Ensure the Elasticsearch index exists with synonym mapping.\"\"\"\n",
    "        mapping = {\n",
    "            \"settings\": {\n",
    "                \"analysis\": {\n",
    "                    \"filter\": {\n",
    "                        \"synonym_filter\": {\n",
    "                            \"type\": \"synonym\",\n",
    "                            \"synonyms_path\": \"analysis/synonyms.txt\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"analyzer\": {\n",
    "                        \"synonym_analyzer\": {\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\n",
    "                                \"lowercase\",\n",
    "                                \"synonym_filter\"\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"id\": {\"type\": \"text\"},\n",
    "                    \"konten\": {\"type\": \"keyword\"},\n",
    "                    \"jenis\": {\"type\": \"keyword\"},\n",
    "                    \"judul\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"synonym_analyzer\"\n",
    "                    },\n",
    "                    \"deskripsi\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"synonym_analyzer\"\n",
    "                    },\n",
    "                    \"title_embeddings_384\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True\n",
    "                    },\n",
    "                    \"mfd\": {\"type\": \"keyword\"},\n",
    "                    \"domain_name\": {\"type\": \"keyword\"},\n",
    "                    \"tgl_rilis\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "                    },\n",
    "                    \"last_update\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "                    },\n",
    "                    \"source\": {\"type\": \"keyword\"},\n",
    "                    \"url\": {\"type\": \"keyword\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            if self.es.indices.exists(index=self.index_name):\n",
    "                logger.warning(f\"Index {self.index_name} exists, deleting to apply synonym mapping\")\n",
    "                self.es.indices.delete(index=self.index_name)\n",
    "            self.es.indices.create(index=self.index_name, body=mapping)\n",
    "            logger.info(f\"Created index {self.index_name} with synonym mapping\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create index {self.index_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_last_indexed_date_for_mfd(self, mfd):\n",
    "        \"\"\"Retrieve the latest last_update date for a specific MFD.\"\"\"\n",
    "        try:\n",
    "            response = self.es.options(ignore_status=[404]).search(\n",
    "                index=self.index_name,\n",
    "                body={\n",
    "                    \"query\": {\"term\": {\"mfd\": mfd}},\n",
    "                    \"aggs\": {\"max_date\": {\"max\": {\"field\": \"last_update\"}}},\n",
    "                    \"size\": 0\n",
    "                }\n",
    "            )\n",
    "            max_date = response['aggregations']['max_date'].get('value_as_string')\n",
    "            if max_date:\n",
    "                logger.info(f\"Latest last_update for MFD {mfd}: {max_date}\")\n",
    "                return max_date\n",
    "            logger.info(f\"No data found for MFD {mfd}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error retrieving last_update for MFD {mfd}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def save_last_indexed_date(self, date):\n",
    "        \"\"\"Save the last indexed date to Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            response = self.es.index(\n",
    "                index=self.index_name,\n",
    "                id='_metadata',\n",
    "                body={'last_indexed_date': date},\n",
    "                refresh=\"true\"\n",
    "            )\n",
    "            logger.info(f\"Updated global last indexed date: {date}, Response: {response.get('result')}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save global last indexed date: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_title(self, title):\n",
    "        \"\"\"Clean title for URL formatting.\"\"\"\n",
    "        if title is None or (isinstance(title, float) and math.isnan(title)):\n",
    "            logger.warning(\"Title is None or NaN, returning empty string\")\n",
    "            return \"\"\n",
    "        title = str(title).strip()\n",
    "        if not title:\n",
    "            logger.warning(\"Title is empty, returning empty string\")\n",
    "            return \"\"\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s-]', '', title)\n",
    "        return title.replace(' ', '-')[:100]\n",
    "\n",
    "    def encode_table_id(self, table_id, table_source):\n",
    "        \"\"\"Encode table ID with table source.\"\"\"\n",
    "        return base64.b64encode(f\"{table_id}#{table_source}\".encode()).decode()\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "        retry=retry_if_exception_type(requests.RequestException),\n",
    "        before_sleep=lambda retry_state: logger.warning(\n",
    "            f\"Retrying API call (attempt {retry_state.attempt_number}/3) due to {retry_state.outcome.exception()}\"\n",
    "        )\n",
    "    )\n",
    "    def fetch_api_response(self, url):\n",
    "        \"\"\"Fetch API response with retries.\"\"\"\n",
    "        logger.debug(f\"Calling API: {url}\")\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    @lru_cache(maxsize=1000)\n",
    "    def mfdName(self, mfd):\n",
    "        \"\"\"Fetch domain URL for given MFD with caching.\"\"\"\n",
    "        if mfd == \"0000\":\n",
    "            return \"https://www.bps.go.id\"\n",
    "\n",
    "        try:\n",
    "            mfd = str(mfd).zfill(4)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            if mfd.endswith(\"00\"):\n",
    "                url = f\"https://webapi.bps.go.id/v1/api/domain/type/prov/key/{self.api_key}/\"\n",
    "                resp = self.fetch_api_response(url)\n",
    "                data_list = resp.get(\"data\", [])\n",
    "                if isinstance(data_list, list) and len(data_list) > 1:\n",
    "                    for item in data_list[1]:\n",
    "                        domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                        if domain_id == mfd:\n",
    "                            return item.get(\"domain_url\")\n",
    "            else:\n",
    "                prov = mfd[:2] + \"00\"\n",
    "                url = f\"https://webapi.bps.go.id/v1/api/domain/type/kabbyprov/key/{self.api_key}/prov/{prov}\"\n",
    "                resp = self.fetch_api_response(url)\n",
    "                data_list = resp.get(\"data\", [])\n",
    "                if isinstance(data_list, list) and len(data_list) > 1:\n",
    "                    for item in data_list[1]:\n",
    "                        domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                        if domain_id == mfd:\n",
    "                            return item.get(\"domain_url\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in mfdName({mfd}): {e}\")\n",
    "        return None\n",
    "\n",
    "    def fetch_domain_names(self):\n",
    "        \"\"\"Fetch domain names from BPS API.\"\"\"\n",
    "        domain_names = {}\n",
    "        try:\n",
    "            prov_url = f\"https://webapi.bps.go.id/v1/api/domain/type/prov/key/{self.api_key}/\"\n",
    "            prov_resp = self.fetch_api_response(prov_url)\n",
    "            prov_data = prov_resp.get(\"data\", [])\n",
    "            if isinstance(prov_data, list) and len(prov_data) > 1:\n",
    "                for item in prov_data[1]:\n",
    "                    domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                    domain_name = item.get(\"domain_name\", \"\")\n",
    "                    if domain_name:\n",
    "                        domain_names[domain_id] = domain_name\n",
    "\n",
    "            for prov_id in domain_names.keys():\n",
    "                if prov_id.endswith(\"00\"):\n",
    "                    kab_url = f\"https://webapi.bps.go.id/v1/api/domain/type/kabbyprov/key/{self.api_key}/prov/{prov_id}\"\n",
    "                    kab_resp = self.fetch_api_response(kab_url)\n",
    "                    kab_data = kab_resp.get(\"data\", [])\n",
    "                    if isinstance(kab_data, list) and len(kab_data) > 1:\n",
    "                        for item in kab_data[1]:\n",
    "                            domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                            domain_name = item.get(\"domain_name\", \"\")\n",
    "                            if domain_name:\n",
    "                                domain_names[domain_id] = domain_name\n",
    "            logger.info(f\"Fetched {len(domain_names)} domain names\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch domain names: {e}\")\n",
    "        return domain_names\n",
    "\n",
    "    def fetch_all_mfds(self):\n",
    "        \"\"\"Fetch all MFDs from BPS API.\"\"\"\n",
    "        mfds = [\"0000\"]\n",
    "        try:\n",
    "            prov_url = f\"https://webapi.bps.go.id/v1/api/domain/type/prov/key/{self.api_key}/\"\n",
    "            prov_resp = self.fetch_api_response(prov_url)\n",
    "            prov_data = prov_resp.get(\"data\", [])\n",
    "            if isinstance(prov_data, list) and len(prov_data) > 1:\n",
    "                for item in prov_data[1]:\n",
    "                    domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                    mfds.append(domain_id)\n",
    "\n",
    "            for prov_id in mfds[1:]:\n",
    "                kab_url = f\"https://webapi.bps.go.id/v1/api/domain/type/kabbyprov/key/{self.api_key}/prov/{prov_id}\"\n",
    "                kab_resp = self.fetch_api_response(kab_url)\n",
    "                kab_data = kab_resp.get(\"data\", [])\n",
    "                if isinstance(kab_data, list) and len(kab_data) > 1:\n",
    "                    for item in kab_data[1]:\n",
    "                        domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                        mfds.append(domain_id)\n",
    "            logger.info(f\"Fetched {len(mfds)} MFDs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch MFDs: {e}\")\n",
    "        return mfds\n",
    "\n",
    "    def format_data(self, row, title_embedding, table_source=\"1\"):\n",
    "        \"\"\"Format a row for Elasticsearch indexing.\"\"\"\n",
    "        domain_id = row.get('domain_id', '0000')\n",
    "        table_id = str(row['table_id'])\n",
    "        doc_id = f\"table_website_{domain_id}_{table_id}\"\n",
    "        domain_url = self.mfdName(domain_id)\n",
    "        encoded_table_id = self.encode_table_id(table_id, table_source)\n",
    "        cleaned_title = self.clean_title(row['title'])\n",
    "        domain_name = self.domain_names.get(domain_id, \"\")\n",
    "\n",
    "        updt_date = row.get('updt_date')\n",
    "        formatted_date = None\n",
    "        if updt_date and updt_date != '':\n",
    "            try:\n",
    "                parsed_date = pd.to_datetime(updt_date)\n",
    "                if pd.notna(parsed_date):\n",
    "                    formatted_date = parsed_date.strftime('%Y-%m-%d')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to format date for table_id {table_id}: {updt_date}, Error: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"_index\": self.index_name,\n",
    "            \"_id\": doc_id,\n",
    "            \"_source\": {\n",
    "                \"id\": table_id,\n",
    "                \"judul\": row['title'],\n",
    "                \"deskripsi\": row['title'],\n",
    "                \"title_embeddings_384\": title_embedding.tolist(),\n",
    "                \"url\": f\"{domain_url}/statistics-table/{table_source}/{encoded_table_id}/{cleaned_title}.html\",\n",
    "                \"tgl_rilis\": None,\n",
    "                \"last_update\": formatted_date,\n",
    "                \"mfd\": domain_id,\n",
    "                \"domain_name\": domain_name,\n",
    "                \"jenis\": \"statictable\",\n",
    "                \"konten\": \"table\",\n",
    "                \"source\": table_source\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def index_dataframe(self, df):\n",
    "        \"\"\"Index DataFrame data into Elasticsearch using bulk.\"\"\"\n",
    "        start_time = time.time()\n",
    "        success_count = 0\n",
    "        failure_count = 0\n",
    "        error_messages = []\n",
    "        batch_titles = []\n",
    "        batch_data = []\n",
    "        latest_date = 0\n",
    "        doc_ids = set()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            updt_date = row.get('updt_date')\n",
    "            item_date = 0\n",
    "            if updt_date and updt_date != '':\n",
    "                try:\n",
    "                    parsed_date = pd.to_datetime(updt_date)\n",
    "                    if pd.notna(parsed_date):\n",
    "                        item_date = parsed_date.timestamp()\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to parse date for table_id {row.get('table_id')}: {updt_date}, Error: {e}\")\n",
    "\n",
    "            title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "            if not title:\n",
    "                logger.warning(f\"Title is empty for table_id: {row.get('table_id')}, skipping\")\n",
    "                continue\n",
    "\n",
    "            doc_id = f\"table_website_{row.get('domain_id', '0000')}_{row.get('table_id')}\"\n",
    "            if doc_id in doc_ids:\n",
    "                logger.warning(f\"Duplicate doc_id detected: {doc_id}, skipping\")\n",
    "                continue\n",
    "            doc_ids.add(doc_id)\n",
    "\n",
    "            batch_titles.append(title)\n",
    "            batch_data.append(row)\n",
    "            latest_date = max(latest_date, item_date)\n",
    "\n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                embeddings = self.embedding_model.encode(batch_titles, show_progress_bar=False)\n",
    "                actions = [\n",
    "                    self.format_data(row, embedding, table_source=\"1\")\n",
    "                    for row, embedding in zip(batch_data, embeddings)\n",
    "                ]\n",
    "                successes, errors = helpers.bulk(self.es, actions, refresh=\"true\")\n",
    "                success_count += successes\n",
    "                failure_count += len(errors)\n",
    "                if errors:\n",
    "                    for error in errors:\n",
    "                        error_msg = f\"ID: {error.get('_id', 'unknown')}, Error: {json.dumps(error)}\"\n",
    "                        logger.error(error_msg)\n",
    "                        error_messages.append(error_msg)\n",
    "                batch_data = []\n",
    "                batch_titles = []\n",
    "                doc_ids.clear()\n",
    "                gc.collect()\n",
    "\n",
    "        if batch_data:\n",
    "            embeddings = self.embedding_model.encode(batch_titles, show_progress_bar=False)\n",
    "            actions = [\n",
    "                self.format_data(row, embedding, table_source=\"1\")\n",
    "                for row, embedding in zip(batch_data, embeddings)\n",
    "            ]\n",
    "            successes, errors = helpers.bulk(self.es, actions, refresh=\"true\")\n",
    "            success_count += successes\n",
    "            failure_count += len(errors)\n",
    "            if errors:\n",
    "                for error in errors:\n",
    "                    error_msg = f\"ID: {error.get('_id', 'unknown')}, Error: {json.dumps(error)}\"\n",
    "                    logger.error(error_msg)\n",
    "                    error_messages.append(error_msg)\n",
    "            gc.collect()\n",
    "\n",
    "        if batch_titles:\n",
    "            sample_text = batch_titles[0]\n",
    "            sample_embedding = self.embedding_model.encode([sample_text], show_progress_bar=False)[0]\n",
    "            logger.info(\n",
    "                f\"Sample embedding verification: text='{sample_text}', \"\n",
    "                f\"embedding_shape={sample_embedding.shape}, embedding_sample={sample_embedding[:5]}\"\n",
    "            )\n",
    "\n",
    "        if success_count > 0 and latest_date > 0:\n",
    "            latest_date_str = datetime.fromtimestamp(latest_date).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            self.save_last_indexed_date(latest_date_str)\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        result = (\n",
    "            f\"Indexing completed. \"\n",
    "            f\"Success: {success_count}, Failed: {failure_count}, Time: {round(execution_time, 2)} seconds. \"\n",
    "            f\"Errors: {'; '.join(error_messages) if error_messages else 'None'}\"\n",
    "        )\n",
    "        logger.info(result)\n",
    "        return result\n",
    "\n",
    "    def fetch_mfd_data(self, mfd, base_url):\n",
    "        \"\"\"Fetch data for a single MFD with optional date filtering.\"\"\"\n",
    "        if not self.bypass_date_filter:\n",
    "            last_indexed_date = self.get_last_indexed_date_for_mfd(mfd)\n",
    "            last_indexed_timestamp = pd.to_datetime(last_indexed_date).timestamp() if last_indexed_date else 0\n",
    "            logger.info(f\"Processing MFD {mfd} with last indexed timestamp: {last_indexed_timestamp}\")\n",
    "        else:\n",
    "            last_indexed_timestamp = 0\n",
    "            logger.info(f\"Processing MFD {mfd} with date filtering bypassed\")\n",
    "\n",
    "        page = 1\n",
    "        mfd_data = []\n",
    "        while True:\n",
    "            try:\n",
    "                url = base_url.format(mfd=mfd, page=page, api_key=self.api_key)\n",
    "                logger.info(f\"Fetching API data for MFD {mfd}, page {page}\")\n",
    "                response = self.fetch_api_response(url)\n",
    "\n",
    "                if response.get(\"status\") != \"OK\" or response.get(\"data-availability\") != \"available\":\n",
    "                    logger.warning(f\"Invalid API response for MFD {mfd}, page {page}: {response.get('status')}\")\n",
    "                    break\n",
    "\n",
    "                metadata = response.get(\"data\", [])[0]\n",
    "                table_data = response.get(\"data\", [])[1]\n",
    "\n",
    "                for item in table_data:\n",
    "                    item[\"domain_id\"] = mfd\n",
    "                    updt_date = item.get(\"updt_date\")\n",
    "                    item_timestamp = 0\n",
    "                    if updt_date and updt_date != '':\n",
    "                        try:\n",
    "                            parsed_date = pd.to_datetime(updt_date, errors='coerce')\n",
    "                            if pd.notna(parsed_date):\n",
    "                                item_timestamp = parsed_date.timestamp()\n",
    "                            else:\n",
    "                                logger.warning(f\"Invalid date format for table_id {item.get('table_id')}: {updt_date}\")\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to parse date for table_id {item.get('table_id')}: {updt_date}, Error: {e}\")\n",
    "\n",
    "                    if self.bypass_date_filter or item_timestamp > last_indexed_timestamp:\n",
    "                        mfd_data.append(item)\n",
    "\n",
    "                total_pages = metadata.get(\"pages\", 1)\n",
    "                logger.info(f\"MFD {mfd}: Page {page} of {total_pages}, retrieved {len(table_data)} records, kept {len(mfd_data)} after date filter\")\n",
    "                if page >= total_pages:\n",
    "                    break\n",
    "                page += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fetch data for MFD {mfd}, page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "        return mfd, mfd_data\n",
    "\n",
    "def fetch_api_data(indexer):\n",
    "    \"\"\"Fetch and index data from BPS Web API with parallel MFD processing.\"\"\"\n",
    "    mfds = indexer.fetch_all_mfds()\n",
    "    base_url = \"https://webapi.bps.go.id/v1/api/list/model/statictable/lang/ind/domain/{mfd}/page/{page}/key/{api_key}/\"\n",
    "    total_success = 0\n",
    "    total_failure = 0\n",
    "    total_errors = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_mfd = {\n",
    "            executor.submit(indexer.fetch_mfd_data, mfd, base_url): mfd\n",
    "            for mfd in mfds\n",
    "        }\n",
    "        for future in as_completed(future_to_mfd):\n",
    "            mfd = future_to_mfd[future]\n",
    "            try:\n",
    "                mfd, mfd_data = future.result()\n",
    "                if mfd_data:\n",
    "                    df = pd.DataFrame(mfd_data, columns=['table_id', 'title', 'updt_date', 'excel', 'domain_id'])\n",
    "                    df = df.drop_duplicates(subset=['table_id', 'domain_id'])\n",
    "                    df = df.dropna(subset=['title'])\n",
    "                    df = df[df['title'].str.strip() != \"\"]\n",
    "                    df['updt_date'] = pd.to_datetime(df['updt_date'], errors='coerce').astype(str).replace('NaT', '')\n",
    "                    df['table_id'] = df['table_id'].astype(str)\n",
    "                    df['domain_id'] = df['domain_id'].astype('category')\n",
    "\n",
    "                    logger.info(f\"Processing {len(df)} records for MFD {mfd}\")\n",
    "                    result = indexer.index_dataframe(df)\n",
    "                    success = int(result.split(\"Success: \")[1].split(\",\")[0])\n",
    "                    failure = int(result.split(\"Failed: \")[1].split(\",\")[0])\n",
    "                    errors = result.split(\"Errors: \")[1].split(\"; \") if \"Errors: \" in result and result.split(\"Errors: \")[1] != \"None\" else []\n",
    "                    total_success += success\n",
    "                    total_failure += failure\n",
    "                    total_errors.extend(errors)\n",
    "                    del df\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    logger.warning(f\"No new data to index for MFD {mfd} after date filtering\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process MFD {mfd}: {e}\")\n",
    "                total_errors.append(f\"MFD {mfd}: {str(e)}\")\n",
    "                total_failure += 1\n",
    "\n",
    "    logger.info(f\"Total indexing: Success={total_success}, Failed={total_failure}, Errors={' '.join(total_errors) if total_errors else 'None'}\")\n",
    "    return total_success, total_failure, total_errors\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set bypass_date_filter=True to index all data, False to use date filtering\n",
    "        indexer = ElasticsearchIndexer(bypass_date_filter=False)\n",
    "        success, failure, errors = fetch_api_data(indexer)\n",
    "        print(f\"Indexing completed. Success: {success}, Failed: {failure}, Errors: {' '.join(errors) if errors else 'None'}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to execute main: {e}\")\n",
    "    finally:\n",
    "        indexer.es.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
