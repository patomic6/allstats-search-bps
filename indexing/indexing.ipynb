{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3346378c",
   "metadata": {},
   "source": [
    "# 1. Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962cc90",
   "metadata": {},
   "source": [
    "Terdapat penyesuaian mapping dari sistem existing, dengan tidak menyertakan field dalam bahasa inggris, karena model multilingual telah mampu mengatasi pencarian lintas bahasa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Inisialisasi koneksi Elasticsearch\n",
    "es = Elasticsearch(\"http://10.100.244.126:9200\")\n",
    "\n",
    "mapping = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"synonym_filter\": {\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms_path\": \"analysis/synonyms.txt\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"synonym_analyzer\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"synonym_filter\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"text\"},\n",
    "      \"konten\": {\"type\": \"keyword\"},\n",
    "      \"jenis\": {\"type\": \"keyword\"},\n",
    "      \"judul\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"synonym_analyzer\"\n",
    "      },\n",
    "      \"deskripsi\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"synonym_analyzer\"\n",
    "      },\n",
    "      \"title_embeddings_384\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True\n",
    "      },\n",
    "      \"mfd\": {\"type\": \"keyword\"},\n",
    "      \"tgl_rilis\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"last_update\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\"\n",
    "      },\n",
    "      \"source\": {\"type\": \"keyword\"},\n",
    "      \"url\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Nama indeks\n",
    "index_name = \"datacontent\"\n",
    "\n",
    "# Hapus indeks jika sudah ada\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Indeks '{index_name}' lama dihapus.\")\n",
    "\n",
    "# Buat indeks baru dengan mapping\n",
    "try:\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"Indeks '{index_name}' berhasil dibuat.\")\n",
    "except Exception as e:\n",
    "    print(\"Gagal membuat indeks:\", e)\n",
    "\n",
    "# (Opsional) Verifikasi mapping\n",
    "try:\n",
    "    current_mapping = es.indices.get_mapping(index=index_name)\n",
    "    print(\"Mapping saat ini:\", current_mapping)\n",
    "except Exception as e:\n",
    "    print(\"Gagal mendapatkan mapping:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403f31b",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc259b4",
   "metadata": {},
   "source": [
    "Inti perubahan indexing dari sistem existing adalah penambahan proses encoding text menjadi dense_vector dan menyimpan hasil encoding tersebut.\n",
    "Pada contoh berikut, dilakukan indexing dari sumber webapi bps. Sumber data dapat disesuaikan dengan kondisi dan perubahan aktual di sistem produksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442907f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ElasticsearchIndexer:\n",
    "    def __init__(self, es_host='http://127.0.0.1:9200', index_name='datacontent'):\n",
    "        self.es = Elasticsearch([es_host])\n",
    "        self.index_name = index_name\n",
    "        self.batch_size = 1000  # Small for debugging\n",
    "        self.api_key = '7a62d6af2de1e805bc5f44d8a0f6ad17'  # Replace with your BPS API key\n",
    "        self.cache_path = \"/cache\"\n",
    "        self.domain_cache = self._load_mfd_cache()\n",
    "        self.mfd_cache = self._load_mfd_cache()\n",
    "        self.embedding_model = SentenceTransformer('yahyaabd/allstats-search-mini-v1-1-mnrl-sts')  # 384-dim embeddings\n",
    "        # self.embedding_model = SentenceTransformer('sentence-transformers/LaBSE')  # 384-dim embeddings\n",
    "        self.domain_names = self.fetch_domain_names()  # Fetch domain names\n",
    "        self.ensure_index()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        \"\"\"Ensure the Elasticsearch index exists with compatible mapping.\"\"\"\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"id\": {\"type\": \"keyword\"},\n",
    "                    \"judul\": {\"type\": \"text\"},\n",
    "                    \"deskripsi\": {\"type\": \"text\"},\n",
    "                    \"title_embeddings_384\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        # \"dims\": 768,\n",
    "                        \"index\": True\n",
    "                    },\n",
    "                    \"mfd\": {\"type\": \"keyword\"},\n",
    "                    \"tgl_rilis\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\":\"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\",\n",
    "                        \"ignore_malformed\": True\n",
    "                    },\n",
    "                    \"last_update\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"yyyy-MM-dd||yyyy/MM/dd||dd-MM-yyyy||dd/MM/yyyy||yyyy-MM-dd HH:mm:ss||epoch_millis\",\n",
    "                        \"ignore_malformed\": True\n",
    "                    },\n",
    "                    \"konten\": {\"type\": \"keyword\"},\n",
    "                    \"jenis\": {\"type\": \"keyword\"},\n",
    "                    \"source\": {\"type\": \"keyword\"},\n",
    "                    \"url\": {\"type\": \"keyword\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            if not self.es.indices.exists(index=self.index_name):\n",
    "                self.es.indices.create(index=self.index_name, body=mapping)\n",
    "                logger.info(f\"Created index {self.index_name}\")\n",
    "            else:\n",
    "                logger.info(f\"Index {self.index_name} already exists, skipping mapping update\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create index {self.index_name}: {e}\")\n",
    "\n",
    "    def get_last_indexed_date(self):\n",
    "        \"\"\"Retrieve the last indexed date from Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            response = self.es.options(ignore_status=[404]).get(index=self.index_name, id='_metadata')\n",
    "            if response.get('found'):\n",
    "                return response['_source'].get('last_indexed_date')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving last indexed date: {e}\")\n",
    "        return None\n",
    "\n",
    "    def save_last_indexed_date(self, date):\n",
    "        \"\"\"Save the last indexed date to Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            self.es.options(refresh=True).index(\n",
    "                index=self.index_name,\n",
    "                id='_metadata',\n",
    "                body={'last_indexed_date': date}\n",
    "            )\n",
    "            logger.info(f\"Updated last indexed date: {date}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save last indexed date: {e}\")\n",
    "\n",
    "    def clean_title(self, title):\n",
    "        \"\"\"Clean title for URL formatting.\"\"\"\n",
    "        if title is None or (isinstance(title, float) and math.isnan(title)):\n",
    "            logger.warning(\"Title is None or NaN, returning empty string\")\n",
    "            return \"\"\n",
    "        title = str(title).strip()\n",
    "        if not title:\n",
    "            logger.warning(\"Title is empty after conversion, returning empty string\")\n",
    "            return \"\"\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s-]', '', title)\n",
    "        return title.replace(' ', '-')[:100]  # Limit to 100 chars\n",
    "\n",
    "    def encode_table_id(self, table_id, table_source):\n",
    "        \"\"\"Encode table ID with table source.\"\"\"\n",
    "        return base64.b64encode(f\"{table_id}#{table_source}\".encode()).decode()\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "        retry=retry_if_exception_type(requests.RequestException),\n",
    "        before_sleep=lambda retry_state: logger.warning(\n",
    "            f\"Retrying API call (attempt {retry_state.attempt_number}/3) due to {retry_state.outcome.exception()}\"\n",
    "        )\n",
    "    )\n",
    "    def fetch_api_response(self, url):\n",
    "        \"\"\"Fetch API response with retries.\"\"\"\n",
    "        logger.debug(f\"Calling API: {url}\")\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        logger.debug(f\"API response status: {response.status_code}\")\n",
    "        return response.json()\n",
    "\n",
    "    def _load_mfd_cache(self):\n",
    "        if os.path.exists(self.cache_path):\n",
    "            with open(self.cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _save_mfd_cache(self):\n",
    "        with open(self.cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.mfd_cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def mfdName(self, mfd):\n",
    "        if mfd == \"0000\":\n",
    "            return \"https://www.bps.go.id\"\n",
    "\n",
    "        try:\n",
    "            mfd = str(mfd).zfill(4)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        # Check from cache\n",
    "        if mfd in self.mfd_cache:\n",
    "            return self.mfd_cache[mfd]\n",
    "\n",
    "        try:\n",
    "            # Domain provinsi\n",
    "            if mfd.endswith(\"00\"):\n",
    "                url = f\"https://webapi.bps.go.id/v1/api/domain/type/prov/key/{self.api_key}/\"\n",
    "                resp = requests.get(url)\n",
    "                if resp.status_code == 200:\n",
    "                    data_list = resp.json().get(\"data\", [])\n",
    "                    if isinstance(data_list, list) and len(data_list) > 1:\n",
    "                        for item in data_list[1]:\n",
    "                            domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                            self.mfd_cache[domain_id] = item.get(\"domain_url\")\n",
    "                        self._save_mfd_cache()\n",
    "                        return self.mfd_cache.get(mfd)\n",
    "\n",
    "            # Domain kabupaten/kota\n",
    "            else:\n",
    "                prov = mfd[:2] + \"00\"\n",
    "                url = f\"https://webapi.bps.go.id/v1/api/domain/type/kabbyprov/key/{self.api_key}/prov/{prov}\"\n",
    "                resp = requests.get(url)\n",
    "                if resp.status_code == 200:\n",
    "                    data_list = resp.json().get(\"data\", [])\n",
    "                    if isinstance(data_list, list) and len(data_list) > 1:\n",
    "                        for item in data_list[1]:\n",
    "                            domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                            self.mfd_cache[domain_id] = item.get(\"domain_url\")\n",
    "                        self._save_mfd_cache()\n",
    "                        return self.mfd_cache.get(mfd)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in mfdName({mfd}): {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fetch_domain_names(self):\n",
    "        \"\"\"Fetch domain names from BPS API for provinces and regencies/cities.\"\"\"\n",
    "        domain_names = {}\n",
    "        try:\n",
    "            # Fetch province domains\n",
    "            prov_url = f\"https://webapi.bps.go.id/v1/api/domain/type/prov/key/{self.api_key}/\"\n",
    "            prov_resp = self.fetch_api_response(prov_url)\n",
    "            prov_data = prov_resp.get(\"data\", [])\n",
    "            if isinstance(prov_data, list) and len(prov_data) > 1:\n",
    "                for item in prov_data[1]:\n",
    "                    domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                    domain_name = item.get(\"domain_name\", \"\")\n",
    "                    if domain_name:\n",
    "                        domain_names[domain_id] = domain_name\n",
    "\n",
    "            # Fetch regency/city domains for each province\n",
    "            for prov_id in domain_names.keys():\n",
    "                if prov_id.endswith(\"00\"):\n",
    "                    kab_url = f\"https://webapi.bps.go.id/v1/api/domain/type/kabbyprov/key/{self.api_key}/prov/{prov_id}\"\n",
    "                    kab_resp = self.fetch_api_response(kab_url)\n",
    "                    kab_data = kab_resp.get(\"data\", [])\n",
    "                    if isinstance(kab_data, list) and len(kab_data) > 1:\n",
    "                        for item in kab_data[1]:\n",
    "                            domain_id = str(item.get(\"domain_id\", \"\")).zfill(4)\n",
    "                            domain_name = item.get(\"domain_name\", \"\")\n",
    "                            if domain_name:\n",
    "                                domain_names[domain_id] = domain_name\n",
    "\n",
    "            logger.info(f\"Fetched {len(domain_names)} domain names\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch domain names: {e}\")\n",
    "        return domain_names\n",
    "\n",
    "    def test_concatenation(self, test_cases):\n",
    "        \"\"\"Test the concatenation logic with provided test cases.\"\"\"\n",
    "        results = []\n",
    "        for title, domain_id, expected in test_cases:\n",
    "            domain_name = self.domain_names.get(domain_id, \"\")\n",
    "            combined_text = f\"{str(title) if pd.notna(title) else ''} {domain_name}\".strip()\n",
    "            passed = combined_text == expected\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"domain_id\": domain_id,\n",
    "                \"combined_text\": combined_text,\n",
    "                \"expected\": expected,\n",
    "                \"passed\": passed\n",
    "            })\n",
    "            logger.info(\n",
    "                f\"Test case: title='{title}', domain_id='{domain_id}', \"\n",
    "                f\"combined_text='{combined_text}', expected='{expected}', passed={passed}\"\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def format_data(self, row, title_embedding, table_source=\"1\"):\n",
    "        \"\"\"Format a DataFrame row for Elasticsearch indexing.\"\"\"\n",
    "        domain_id = row.get('domain_id', '0000')\n",
    "        domain_url = self.mfdName(domain_id)\n",
    "        encoded_table_id = self.encode_table_id(row['table_id'], table_source)\n",
    "        cleaned_title = self.clean_title(row['title'])\n",
    "        \n",
    "        # Format last_update to match mapping (yyyy-MM-dd or None)\n",
    "        updt_date = row.get('updt_date')\n",
    "        formatted_date = None\n",
    "        if updt_date and updt_date != '':\n",
    "            try:\n",
    "                parsed_date = pd.to_datetime(updt_date)\n",
    "                if pd.notna(parsed_date):\n",
    "                    formatted_date = parsed_date.strftime('%Y-%m-dd')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to format date for table_id {row.get('id')}: {updt_date}, Error: {e}\")\n",
    "\n",
    "        formatted = {\n",
    "            \"corpus_id\": str(row['corpus_id']),\n",
    "            \"id\": str(row['table_id']),\n",
    "            \"judul\": row['title'],\n",
    "            \"deskripsi\": row['title'],\n",
    "            \"title_embeddings_384\": title_embedding.tolist(),\n",
    "            \"url\": f\"{domain_url}/statistics-table/{table_source}/{encoded_table_id}/{cleaned_title}.html\",\n",
    "            \"tgl_rilis\": None,\n",
    "            \"last_update\": formatted_date,\n",
    "            \"mfd\": domain_id,\n",
    "            \"jenis\": \"statictable\",\n",
    "            \"konten\": \"table\",\n",
    "            \"source\": table_source\n",
    "        }\n",
    "        logger.debug(f\"Formatted document: {formatted}\")\n",
    "        return formatted\n",
    "\n",
    "    def index_dataframe(self, df):\n",
    "        \"\"\"Index DataFrame data into Elasticsearch with concatenation checks.\"\"\"\n",
    "        start_time = time.time()\n",
    "        success_count = 0\n",
    "        failure_count = 0\n",
    "        error_messages = []\n",
    "        batch_data = []\n",
    "        batch_titles = []\n",
    "        latest_date = 0\n",
    "        sample_combined_texts = []  # Store samples for inspection\n",
    "\n",
    "        last_indexed_date = self.get_last_indexed_date()\n",
    "        last_indexed_timestamp = pd.to_datetime(last_indexed_date).timestamp() if last_indexed_date else 0\n",
    "        logger.info(f\"Last indexed timestamp: {last_indexed_timestamp}\")\n",
    "\n",
    "        # Check for duplicate table_id\n",
    "        duplicates = df['corpus_id'].duplicated().sum()\n",
    "        logger.info(f\"Duplicate table_id count: {duplicates}\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            item_date = 0\n",
    "            updt_date = row.get('updt_date')\n",
    "            if updt_date and updt_date != '':\n",
    "                try:\n",
    "                    parsed_date = pd.to_datetime(updt_date)\n",
    "                    if pd.notna(parsed_date):\n",
    "                        item_date = parsed_date.timestamp()\n",
    "                    else:\n",
    "                        logger.warning(f\"Invalid date format for table_id {row.get('table_id')}: {updt_date}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to parse date for table_id {row.get('table_id')}: {updt_date}, Error: {e}\")\n",
    "\n",
    "            # Temporarily disable date filter\n",
    "            logger.debug(f\"Processing table_id: {row.get('table_id')}, updt_date: {updt_date}\")\n",
    "            domain_id = row.get('domain_id', '0000')\n",
    "            domain_name = self.domain_names.get(domain_id, \"\")\n",
    "            title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "            combined_text = f\"{title} {domain_name}\".strip()\n",
    "\n",
    "            # Log the concatenated text for verification\n",
    "            logger.debug(f\"table_id: {row.get('table_id')}, title: '{title}', domain_name: '{domain_name}', combined_text: '{combined_text}'\")\n",
    "\n",
    "            # Validation: Warn if combined_text is empty or unexpected\n",
    "            if not combined_text:\n",
    "                logger.warning(f\"Empty combined_text for table_id: {row.get('table_id')}, title: '{title}', domain_id: '{domain_id}'\")\n",
    "            elif not title:\n",
    "                logger.warning(f\"Title is empty for table_id: {row.get('table_id')}, domain_id: '{domain_id}', combined_text: '{combined_text}'\")\n",
    "\n",
    "            # Collect samples for inspection (first 5 rows)\n",
    "            if idx < 5:\n",
    "                sample_combined_texts.append({\n",
    "                    \"table_id\": row.get('table_id'),\n",
    "                    \"title\": title,\n",
    "                    \"domain_id\": domain_id,\n",
    "                    \"domain_name\": domain_name,\n",
    "                    \"combined_text\": combined_text\n",
    "                })\n",
    "\n",
    "            batch_titles.append(combined_text)\n",
    "            batch_data.append(row)\n",
    "            latest_date = max(latest_date, item_date)\n",
    "\n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                embeddings = self.embedding_model.encode(batch_titles, show_progress_bar=False)\n",
    "                logger.debug(f\"Generated embeddings for batch: shape={embeddings.shape}\")\n",
    "                \n",
    "                formatted_batch = [\n",
    "                    self.format_data(row, embedding, table_source=\"1\")\n",
    "                    for row, embedding in zip(batch_data, embeddings)\n",
    "                ]\n",
    "                success, errors = self.process_batch(formatted_batch)\n",
    "                success_count += success\n",
    "                failure_count += len(errors)\n",
    "                error_messages.extend(errors)\n",
    "                batch_data = []\n",
    "                batch_titles = []\n",
    "\n",
    "        if batch_data:\n",
    "            embeddings = self.embedding_model.encode(batch_titles, show_progress_bar=False)\n",
    "            logger.debug(f\"Generated embeddings for final batch: shape={embeddings.shape}\")\n",
    "            formatted_batch = [\n",
    "                self.format_data(row, embedding, table_source=\"1\")\n",
    "                for row, embedding in zip(batch_data, embeddings)\n",
    "            ]\n",
    "            success, errors = self.process_batch(formatted_batch)\n",
    "            success_count += success\n",
    "            failure_count += len(errors)\n",
    "            error_messages.extend(errors)\n",
    "\n",
    "        # Print sample combined texts for manual inspection\n",
    "        logger.info(\"Sample concatenated texts (first 5 rows):\")\n",
    "        for sample in sample_combined_texts:\n",
    "            logger.info(\n",
    "                f\"table_id: {sample['table_id']}, title: '{sample['title']}', \"\n",
    "                f\"domain_id: '{sample['domain_id']}', domain_name: '{sample['domain_name']}', \"\n",
    "                f\"combined_text: '{sample['combined_text']}'\"\n",
    "            )\n",
    "\n",
    "        # Verify embeddings for a sample\n",
    "        if sample_combined_texts:\n",
    "            sample_text = sample_combined_texts[0]['combined_text']\n",
    "            sample_embedding = self.embedding_model.encode([sample_text], show_progress_bar=False)[0]\n",
    "            logger.info(\n",
    "                f\"Sample embedding verification: text='{sample_text}', \"\n",
    "                f\"embedding_shape={sample_embedding.shape}, embedding_sample={sample_embedding[:5]}\"\n",
    "            )\n",
    "\n",
    "        if success_count > 0 and latest_date > last_indexed_timestamp:\n",
    "            latest_date_str = datetime.fromtimestamp(latest_date).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            self.save_last_indexed_date(latest_date_str)\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        result = (\n",
    "            f\"Indexing completed. \"\n",
    "            f\"Success: {success_count}, Failed: {failure_count}, Time: {round(execution_time, 2)} seconds. \"\n",
    "            f\"Errors: {'; '.join(error_messages) if error_messages else 'None'}\"\n",
    "        )\n",
    "        logger.info(result)\n",
    "        return result\n",
    "\n",
    "    def process_batch(self, batch_data):\n",
    "        \"\"\"Process a batch of data for Elasticsearch bulk indexing.\"\"\"\n",
    "        actions = []\n",
    "        for item in batch_data:\n",
    "            action = {\n",
    "                \"update\": {\n",
    "                    \"_index\": self.index_name,\n",
    "                    \"_id\": item[\"corpus_id\"]\n",
    "                }\n",
    "            }\n",
    "            actions.append(action)\n",
    "            actions.append({\n",
    "                \"doc\": item,\n",
    "                \"doc_as_upsert\": True\n",
    "            })\n",
    "    \n",
    "        bulk_payload = '\\n'.join(json.dumps(action) for action in actions) + '\\n'\n",
    "        logger.debug(f\"Bulk payload (first 2000 chars):\\n{bulk_payload[:2000]}\")\n",
    "    \n",
    "        logger.info(f\"Sending {len(actions)//2} documents to Elasticsearch\")\n",
    "        success_count = 0\n",
    "        error_messages = []\n",
    "        try:\n",
    "            response = self.es.options(request_timeout=30).bulk(body=bulk_payload)\n",
    "            response_body = response.body\n",
    "            logger.debug(f\"Bulk response: {json.dumps(response_body, indent=2)}\")\n",
    "    \n",
    "            if response_body.get('errors', False):\n",
    "                for item in response_body.get('items', []):\n",
    "                    if 'update' in item:\n",
    "                        update = item['update']\n",
    "                        if update.get('result') in ['created', 'updated']:\n",
    "                            success_count += 1\n",
    "                        elif 'error' in update:\n",
    "                            error_msg = f\"ID: {update['_id']}, Error: {json.dumps(update['error'])}\"\n",
    "                            logger.error(error_msg)\n",
    "                            error_messages.append(error_msg)\n",
    "                        else:\n",
    "                            error_msg = f\"ID: {update['_id']}, Unknown error: {json.dumps(update)}\"\n",
    "                            logger.error(error_msg)\n",
    "                            error_messages.append(error_msg)\n",
    "            else:\n",
    "                success_count = len(response_body.get('items', []))\n",
    "    \n",
    "            logger.info(f\"Bulk result: Success={success_count}, Errors={len(error_messages)}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Bulk indexing failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            error_messages.append(error_msg)\n",
    "            return 0, error_messages\n",
    "    \n",
    "        return success_count, error_messages\n",
    "\n",
    "def main():\n",
    "    csv_file = 'statictable-all-cleaned.csv'  # Replace with your CSV file path\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.drop_duplicates(subset=['title'])\n",
    "        df = df.dropna(subset=['title'])  # Hapus baris dengan title NaN\n",
    "        df = df[df['title'].str.strip() != \"\"]  # Hapus baris dengan title berupa string kosong setelah di-trim\n",
    "\n",
    "        df['updt_date'] = pd.to_datetime(df['updt_date'], errors='coerce').astype(str).replace('NaT', '')\n",
    "        df['table_id'] = df['table_id'].astype(str)\n",
    "        df['domain_id'] = df['domain_id'].astype(str).str.zfill(4)  # Ensure domain_id is 4 digits\n",
    "        logger.info(f\"Loaded CSV with {len(df)} rows\")\n",
    "        logger.info(f\"Unique domain_id values: {df['domain_id'].unique().tolist()}\")\n",
    "        print(df[['corpus_id', 'title', 'updt_date', 'domain_id']].head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    indexer = ElasticsearchIndexer()\n",
    "\n",
    "    # Run concatenation tests\n",
    "    test_cases = [\n",
    "        (\"Statistik Penduduk\", \"1100\", \"Statistik Penduduk Aceh\"),  # Normal case\n",
    "        (\"Data Ekonomi\", \"1101\", \"Data Ekonomi Simeulue\"),         # Regency case\n",
    "        (\"\", \"1100\", \"Aceh\"),                                     # Empty title\n",
    "        (None, \"1100\", \"Aceh\"),                                   # None title\n",
    "        (\"Nasional\", \"0000\", \"Nasional\"),                         # No domain_name\n",
    "    ]\n",
    "    test_results = indexer.test_concatenation(test_cases)\n",
    "    print(\"Concatenation Test Results:\")\n",
    "    for result in test_results:\n",
    "        print(\n",
    "            f\"title='{result['title']}', domain_id='{result['domain_id']}', \"\n",
    "            f\"combined_text='{result['combined_text']}', expected='{result['expected']}', \"\n",
    "            f\"passed={result['passed']}\"\n",
    "        )\n",
    "\n",
    "    result = indexer.index_dataframe(df)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
